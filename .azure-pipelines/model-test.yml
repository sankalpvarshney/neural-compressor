trigger: none

pr:
  autoCancel: true
  drafts: false
  branches:
    include:
      - master
  paths:
    include:
      - neural_compressor
      - setup.py
      - .azure-pipelines/model-test.yml
      - .azure-pipelines/scripts/models
      - examples/tensorflow/oob_models/quantization/ptq
    exclude:
      - neural_compressor/ux

pool: MODEL_PERF_TEST_TF

variables:
  OUT_SCRIPT_PATH: $(Build.SourcesDirectory)/.azure-pipelines/scripts/models
  SCRIPT_PATH: /neural-compressor/.azure-pipelines/scripts

parameters:
  - name: TensorFlow_Model
    displayName: Run TensorFlow models?
    type: boolean
    default: true
  - name: PyTorch_Model
    displayName: Run PyTorch models?
    type: boolean
    default: true
  - name: ONNX_Model
    displayName: Run ONNX models?
    type: boolean
    default: true
  - name: MXNet_Model
    displayName: Run MXNet models?
    type: boolean
    default: true

  - name: TensorFlowModelList
    type: object
    default:
      - resnet50v1.5
      - ssd_resnet50_v1
      - ssd_mobilenet_v1_ckpt
      - inception_v1
      - darknet19
      - densenet-121
      - resnet-101
  - name: PyTorchModelList
    type: object
    default:
      - resnet18
      - resnet18_fx
  - name: ONNXModelList
    type: object
    default:
      - resnet50-v1-12
      # - bert_base_MRPC_dynamic
  - name: MXNetModelList
    type: object
    default:
      - resnet50v1

stages:
  - stage: TensorFlowModels
    displayName: Run TensorFlow Model
    pool: MODEL_PERF_TEST_TF
    dependsOn: []
    condition: and(succeeded(), eq('${{ parameters.TensorFlow_Model }}', 'true'))
    jobs:
      - ${{ each model in parameters.TensorFlowModelList }}:
          - job:
            displayName: ${{ model }}
            steps:
              - template: template/model-template.yml
                parameters:
                  modelName: ${{ model }}
                  framework: "tensorflow"

  - stage: PyTorchModels
    displayName: Run PyTorch Model
    pool: MODEL_PERF_TEST
    dependsOn: []
    condition: and(succeeded(), eq('${{ parameters.PyTorch_Model }}', 'true'))
    jobs:
      - ${{ each model in parameters.PyTorchModelList }}:
          - job:
            displayName: ${{ model }}
            steps:
              - template: template/model-template.yml
                parameters:
                  modelName: ${{ model }}
                  framework: "pytorch"

  - stage: ONNXModels
    displayName: Run ONNX Model
    pool: MODEL_PERF_TEST
    dependsOn: []
    condition: and(succeeded(), eq('${{ parameters.ONNX_Model }}', 'true'))
    jobs:
      - ${{ each model in parameters.ONNXModelList }}:
          - job:
            displayName: ${{ model }}
            steps:
              - template: template/model-template.yml
                parameters:
                  modelName: ${{ model }}
                  framework: "onnxrt"

  - stage: MXNetModels
    displayName: Run MXNet Model
    pool: MODEL_PERF_TEST
    dependsOn: []
    condition: and(succeeded(), eq('${{ parameters.MXNet_Model }}', 'true'))
    jobs:
      - ${{ each model in parameters.MXNetModelList }}:
          - job:
            displayName: ${{ model }}
            steps:
              - template: template/model-template.yml
                parameters:
                  modelName: ${{ model }}
                  framework: "mxnet"

  - stage: GenerateLogs
    displayName: Generate Report
    pool:
      vmImage: "ubuntu-latest"
    dependsOn: [TensorFlowModels, PyTorchModels, MXNetModels, ONNXModels]
    jobs:
      - job: GenerateReport
        steps:
          - script: |
              echo ${BUILD_SOURCESDIRECTORY}
              rm -fr ${BUILD_SOURCESDIRECTORY} || sudo rm -fr ${BUILD_SOURCESDIRECTORY} || true
              echo y | docker system prune
            displayName: "Clean workspace"
          - checkout: self
            clean: true
            displayName: "Checkout out Repo"
          - task: DownloadPipelineArtifact@2
            inputs:
              artifact:
              patterns: "**/*_summary.log"
              path: $(OUT_SCRIPT_PATH)
          - task: DownloadPipelineArtifact@2
            inputs:
              artifact:
              patterns: "**/*_tuning_info.log"
              path: $(OUT_SCRIPT_PATH)
          - task: UsePythonVersion@0
            displayName: "Use Python 3.8"
            inputs:
              versionSpec: "3.8"
          - script: |
              cd ${OUT_SCRIPT_PATH}
              mkdir generated
              mkdir last_generated
              pip install requests
              python -u collect_log_all.py --logs_dir $(OUT_SCRIPT_PATH) --output_dir generated --build_id=$(Build.BuildId)
            displayName: "Collect all logs"
          - task: DownloadPipelineArtifact@2
            continueOnError: true
            inputs:
              source: "specific"
              artifact: "FinalReport"
              patterns: "**.log"
              path: $(OUT_SCRIPT_PATH)/last_generated
              project: $(System.TeamProject)
              pipeline: "Model-Test"
              runVersion: "specific"
              runId: $(refer_buildId)
            displayName: "Download last logs"
          - script: |
              echo "------ Generating final report.html ------"
              cd ${OUT_SCRIPT_PATH}
              /usr/bin/bash generate_report.sh --WORKSPACE generated --output_dir generated --last_logt_dir last_generated
            displayName: "Generate report"
          - task: PublishPipelineArtifact@1
            inputs:
              targetPath: $(OUT_SCRIPT_PATH)/generated
              artifact: FinalReport
              publishLocation: "pipeline"
            displayName: "Publish report"
          - script: |
              if [ $(is_perf_reg) == 'true' ]; then
                echo "[Performance Regression] Some model performance regression occurred, please check artifacts and reports."
                exit 1
              fi
            displayName: "Specify performance regression"
