"use strict";
let str = '# this is the beginning of a single code snippet\nimport glob\nimport torch\nimport os\nimport sys\nfrom tqdm import tqdm\nfrom dalle_pytorch import VQGanVAE, DALLE, DiscreteVAE\nfrom dalle_pytorch.tokenizer import tokenizer\nfrom einops import repeat\nfrom dalle_nc import DALLE, DiscreteVAE\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\n\n# model\nvae = DiscreteVAE(\n    image_size = 8,\n    num_layers = 3,\n    num_tokens = 8192,\n    codebook_dim = 1024,\n    hidden_dim = 64,\n    num_resnet_blocks = 1,\n    temperature = 0.9\n)\n\ndalle = DALLE(\n    dim = 1024,\n    vae = vae,                  # automatically infer (1) image sequence length and (2) number of image tokens\n    num_text_tokens = 100000,    # vocab size for text\n    text_seq_len = 256,         # text sequence length\n    depth = 12,                 # should aim to be 64\n    heads = 16,                 # attention heads\n    dim_head = 64,              # attention head dimension\n    attn_dropout = 0.1,         # attention dropout\n    ff_dropout = 0.1            # feedforward dropout\n)\n# [NeuralCoder] pytorch_inc_dynamic_quant for dalle [Beginning Line]\nif "GraphModule" not in str(type(dalle)):\n    from neural_compressor.conf.config import QuantConf\n    from neural_compressor.experimental import Quantization, common\n    quant_config = QuantConf()\n    quant_config.usr_cfg.quantization.approach = "post_training_dynamic_quant"\n    quant_config.usr_cfg.model.framework = "pytorch"\n    quantizer = Quantization(quant_config)\n    quantizer.model = common.Model(dalle)\n    dalle = quantizer()\n    dalle = dalle.model\n    dalle.eval()\n# [NeuralCoder] pytorch_inc_dynamic_quant for dalle [Ending Line]\n\ndalle.eval()\n\n# real data for DALLE image generation\nfiles = glob.glob(\'/home2/longxin/neural_compressor_ext_lab/real_text.txt\')\n\n# create dataloader\ninput_list = []\nwith torch.no_grad():\n    count = 0\n    for file in files:\n        texts = open(file, \'r\').read().split(\'\\n\')\n        for text in texts:\n            print(text)\n\n            num_images = 1\n\n            top_k = 0.9\n\n            image_size = vae.image_size\n\n            texts = text.split(\'|\')\n\n            for j, text in tqdm(enumerate(texts)):\n                text_tokens = tokenizer.tokenize([text], 256).to(\'cpu\')\n\n                text_tokens = repeat(text_tokens, \'() n -> b n\', b=num_images)\n\n                for text_chunk in tqdm(text_tokens):\n                    d = {}\n                    d["text"] = text_chunk\n                    d["filter_thres"] = top_k\n                    input_list.append(d)\n\nclass MyDataset(Dataset):\n    def __init__(self):\n        self.samples = input_list\n\n    def __getitem__(self, idx):\n        return self.samples[idx], 1\n\n    def __len__(self):\n        return len(self.samples)\ndataset = MyDataset()\ndataloader = DataLoader(dataset)\n\n# inference\nwith torch.no_grad():\n    for step, (inputs, labels) in enumerate(dataloader):\n        print("running inference ...")\n        output = dalle(**inputs)\n\n';
let str1 = str.split('# this is the beginning of a single code snippet\\n').length;
console.log(__filename);
